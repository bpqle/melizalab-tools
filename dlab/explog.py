#!/usr/bin/env python
# -*- coding: iso-8859-1 -*-
"""
module for reading explog files. These files contain a bunch of very
essential meta-data generated during data acquisition by saber, in particular
time offsets for triggers and stimulus presentation
"""

import tables as t
import re, sys, os
    

class explog(object):
    """
    This class manages data associated with an explog file.  The
    text explog generated by saber is parsed into tables of entries
    and stimuli.  The basic grouping is by recording site; all the
    entries from a particular site are assumed to come from the same
    cells.  The recordings of this cell may be split across multiple
    files, if there are multiple channels or if the number of entries
    exceeds the size limit of the pcm_seq2 format.  Having these tables
    allows the entries associated with a site to be accessed with a simple
    monotonic index.
    """

    def __init__(self, logfile):
        if logfile.endswith('.h5'):
            self.elog = t.openFile(logfile, 'r')
        else:
            self.elog = readexplog(logfile, logfile + '.h5')

        self.__cache_channels()

    def __del__(self):
        self.elog.close()

    def __cache_channels(self):
        """
        Determines the channel set.
        """
        self._channels = []
        table = self.elog.root.channels
        for r in table:
            self._channels.append(r['name'])

    @property
    def nchannels(self):
        return len(self._channels)

    def getsiteentry(self, site, entry, channels=None):
        """
        Looks up the records associated with a particular site and siteentry
        """
        if channels==None:
            channels = range(self.nchannels)
        table = self.elog.root.entries
        pen,site = site
        rnums = [r.nrow for r in table.where(table.cols.siteentry==entry) \
                 if r['pen']==pen and r['site']==site and r['channel'] in channels]
        if len(rnums)==0:
            raise ValueError, "No entries match the entry and channel set"

        return table.readCoordinates(rnums)
        
    def getentrytimes(self, site):
        """
        The mapping between siteentry and abstime should be one-to-one
        for any site, so we can use any of the channels.  It's still
        kind of an expensive operation for huge explogs, so try to
        only run it once.
        """
        pen,site = site        
        table = self.elog.root.entries
        rnums = [r.nrow for r in table.where(table.cols.site==site) \
                 if r['pen']==pen and r['channel']==0]
        if len(rnums)==0:
            raise ValueError, "No entries for channel 0; something is wrong"

        rows =  table.readCoordinates(rnums)
        out  = {}
        for r in rows:
            out[r['siteentry']] = r['abstime']
        return out

    def _get_site(self):
        return self._site

    def _set_site(self, site):
        self._site = (int(site[0]), int(site[1]))

    site = property(_get_site, _set_site, None, "The current recording site")

    def __getchannels(self):
        """
        Determines the channel set.
        """
        self._channels = []
        table = self.elog.root.channels
        for r in table:
            self._channels.append(r['name'])

    @property
    def nchannels(self):
        return len(self._channels)


    
_reg_create = re.compile(r"'(?P<file>(?P<base>\w+)_\w+.pcm_seq2)' (?P<action>created|closed)")
_reg_triggeron = re.compile(r"TRIG_ON, (?P<chan>\w+):entry (?P<entry>\d*) \((?P<onset>\d*)\)")
_reg_triggeroff = re.compile(r"TRIG_OFF, (?P<chan>\w+):entry (?P<entry>\d*), wrote (?P<samples>\d*)")
_reg_stimulus = re.compile(r"stimulus: REL:(?P<rel>[\d\.]*) ABS:(?P<abs>\d*) NAME:'(?P<stim>\S*)'")
_reg_site = re.compile(r"site (?P<site>\d*)")
_reg_pen  = re.compile(r"pen (?P<pen>\d*)")


class Entries(t.IsDescription):
    """
    Table for entries (episodes)
    """
    filebase = t.StringCol(128, pos=0)
    channel  = t.UInt16Col(pos=1)
    pen      = t.UInt16Col(pos=2)
    site     = t.UInt16Col(pos=3)
    entry    = t.UInt16Col(pos=4)
    siteentry = t.UInt32Col(pos=5)
    abstime  = t.UInt64Col(pos=6)
    duration = t.UInt32Col(pos=7)

class Stimuli(t.IsDescription):
    abstime  = t.UInt64Col(pos=0)
    name     = t.StringCol(128, pos=1)
    entrytime= t.UInt64Col(pos=2)

class Channels(t.IsDescription):
    name     = t.StringCol(64)


def readexplog(logfile, outfile, site_sort=False):
    """
    Parses episode information from the explog. Generates an h5
    file with the entry data in one table and the stimulus data in
    another.

    site_sort - if true, makes a directory for each pen/site and moves
                the files into that directory.
    """

    expdir = os.path.dirname(logfile)
    channels = []
    files  = {}
    triggers = {}
    currentpen   = 0
    currentsite  = 0
    lastentry    = 0
    entryoffset  = 0
    lastabs      = 0
    absoffset    = 0

    # we scan through the file looking for pcm_seq2 files being opened; this
    # gives us the root filenames that we'll use in generating the indexed
    # label files. Trigger events specify the current entry. The stimulus events
    # don't get logged between TRIG_ON and TRIG_OFF lines, so we have to reconstruct
    # which entry they belong to later. So two tables are created, one keyed by
    # filename/entry with the start and stop times of each recording, and the
    # other keyed by the absolute start time of the stimuli


    fp = open(logfile)
    line_num = 0

    h5 = t.openFile(outfile, mode='w', title="parsed explog %s" % logfile,
                    filters=t.Filters(complevel=1, complib='zlib'))
    entries = _maketable(h5, h5.root,'entries',Entries)
    stimuli = _maketable(h5, h5.root,'stimuli',Stimuli)
    chantable = _maketable(h5, h5.root,'channels',Channels)

    for line in fp:
        line_num += 1

        if line.startswith("FFFF"):
            m1 = _reg_create.search(line)
            if m1:
                if m1.group('action')=='created':
                    # we try to locate the file either in the current directory
                    # or in a subdirectory site_<pen>_<site>
                    seqfile = m1.group('file')
                    subdir = "site_%d_%d" % (currentpen, currentsite)
                    if site_sort and os.path.exists(seqfile):
                        if not os.path.exists(subdir): os.mkdir(subdir)
                        print "%s -> %s" % (seqfile, subdir)
                        os.system("mv %s %s " % (seqfile, subdir))
                    if os.path.exists(os.path.join(subdir, seqfile)):
                        seqfile = os.path.join(subdir, seqfile)
                    files[m1.group('base')] = seqfile
                    entryoffset = lastentry
                else:
                    files.pop(m1.group('base'))
            else:
                print "parse error: Unparseable FFFF line (%d): %s" % (line_num, line)

        # new pen or new site
        if line.startswith("IIII"):
            m1 = _reg_pen.search(line)
            m2 = _reg_site.search(line)
            if m1 or m2:
                lastentry    = 0
            if m1:
                currentpen = int(m1.group('pen'))
            elif m2:
                currentsite = int(m2.group('site'))

        # when saber quits or stop/starts, the abstime gets reset. Since stimuli are matched with triggers
        # by abstime, this can result in stimuli getting assigned to episodes deep in the past
        # The workaround for this is to maintain an offset that gets set to the most recent
        # abstime whenever a quit event is detected
        if line.startswith('%%%%'):
            if line.rstrip().endswith('start'):
                absoffset = lastabs
            else:
                o = line.find('add')
                if o > 0:
                    cname = line[o+4:-1]
                    try:
                        channels.index(cname)
                    except ValueError:
                        channels.append(cname)


        # trigger lines
        if line.startswith("TTTT"):
            m1 = _reg_triggeron.search(line)
            m2 = _reg_triggeroff.search(line)
            if m1:
                # trigger on
                time_trig = int(m1.group('onset')) + absoffset
                triggers[(m1.group('chan'), int(m1.group('entry')))] = time_trig
                lastabs = time_trig
                #print "Entry %d starts %d" % (currententry, time_trig)
            elif m2:
                # trigger off
                key = (m2.group('chan'), int(m2.group('entry')))
                try:
                    trig_on = triggers[key]
                    n_samples = int(m2.group('samples'))
                    # check that we have a file defined in the files dict
                    if not files.has_key(key[0]):
                        print "parse error: found entry %s:%d but no file (line %d)" % \
                          (key + (line_num,))
                    else:
                        lastentry = entryoffset + key[1]
                        row = entries.row
                        row['filebase'] = files[key[0]]
                        row['channel'] = channels.index(key[0])
                        row['pen'] = currentpen
                        row['site'] = currentsite
                        row['entry'] = key[1]
                        row['siteentry'] = lastentry
                        row['abstime'] = trig_on
                        row['duration'] = n_samples
                        row.append()
                        triggers.pop(key)

                except KeyError:
                    print "parse error: found TRIG_OFF for %s:%s, but missed TRIG_ON (line %d)" % \
                          (key + (line_num,))
                except IndexError:
                    print "Unable to parse channel number from %s (line %d)" % (key[0] % line_num)
                    triggers.remove(key)
            else:
                    print "parse error: Unparseable TTTT line (%d): %s" % (line_num, line)


        # stimulus lines
        if line.startswith("QQQQ"):
            m1 = _reg_stimulus.search(line)
            if m1:
                time_stim_rel = float(m1.group('rel'))
                time_stim_abs = int(m1.group('abs')) + absoffset
                lastabs = time_stim_abs
                stimname = m1.group('stim')
                # is it only triggered stimuli that start with "File="?
                # I'm going to leave this as general as possible at some memory cost;
                # the untriggered stimuli will get discarded
                if stimname.startswith('File='):
                    stimname = stimname[5:]

                row = stimuli.row
                row['abstime'] = time_stim_abs
                row['name'] = stimname
                row.append()
            else:
                print "parse error: Unparseable QQQQ line: %s" % line

    # done parsing file
    fp.close()

    for c in channels:
        row = chantable.row
        row['name'] = c
        row.append()

    # sync entries
    h5.flush()
    assignstimuli(h5)


    return h5

# end readexplog()

def assignstimuli(h5, cull_unrecorded=True):
    """
    Sets the stimulus entrytime values to match the corresponding
    entries in the entry table.

    If cull_unrecorded is true, the stimulus records that don't
    correspond to any recorded entry are removed from the database
    """
    entries = h5.root.entries

    coords = [r.nrow for r in entries.where(entries.cols.channel==0)]
    start_times = entries.col('abstime')[coords]
    stop_times = entries.col('duration')[coords] + start_times      

    i = 0
    stimuli = h5.root.stimuli
    for stimulus in stimuli:
        atime = stimulus['abstime']
        while i < len(stop_times):
            if atime < start_times[i]:
                stimulus['entrytime'] = 0L
                stimulus.update()
                break
            elif atime < stop_times[i]:
                stimulus['entrytime'] = long(start_times[i])
                stimulus.update()
                break
            else:
                i += 1

    stimuli.flush()
    
    if cull_unrecorded:
        # easiest just to copy the valid rows to a new table
        valid = stimuli.getWhereList(stimuli.cols.entrytime!=0)
        stimuli.rename('oldstimuli')
        newstimuli = _maketable(h5, '/', 'stimuli', Stimuli)
        newstimuli.append(stimuli[:][valid])
        newstimuli.flush()
        stimuli.remove()

    h5.flush()

def _maketable(file, base, name, descr):
    tbl = file.createTable(base, name, descr)
    tbl.flavor = 'numpy'
    return tbl



if __name__=="__main__":

    test_file = '../data/test.explog'
    if os.path.exists(test_file + ".h5"):
        os.remove(test_file + ".h5")
    z = explog(test_file)
