#!/usr/bin/env python
# -*- coding: iso-8859-1 -*-
"""
module for reading explog files. These files contain a bunch of very
essential meta-data generated during data acquisition by saber, in particular
time offsets for triggers and stimulus presentation
"""

import tables as t
import scipy as nx
import re, sys, os
    
__all__ = ['explog','readexplog']

class explog(object):
    """
    This class manages data associated with an explog file.  The
    text explog generated by saber is parsed into a number of tables.

    Recording is assumed to be episodic; that is, recording on one
    or more channels is triggered at a given time.  Episodes do not
    need to be the same length, but all the channels are assumed to
    be triggered together and have the same length.

    Saber stores raw pcm data in pcm_seq2 files, which can have multiple
    entries.  Recordings are split across files if the size of the
    file exceeds some limit, and each channel has its own file.

    While recordings are being made, stimuli are often presented to the
    animal.  Saber records the times of stimulus presentation in an
    asynchronous manner, so there is no a priori match between an episode
    and a stimulus.  However, in my experiments a single stimulus
    is played in each episode, so it's possible to reconstruct (based on
    the times) which stimuli belong with which episode.
    """

    samplerate = 20

    def __init__(self, logfile, mode='r'):
        """
        Initialize the explog object with an hdf5 file. To
        generate an explog object from a raw saber explog, use
        the module-level function readexplog()
        """
        if isinstance(logfile, t.File):
            self.elog = logfile
        else:
            self.elog = t.openFile(logfile, mode)

    def __del__(self):
        if hasattr(self, 'elog'):
            self.elog.close()

    def getentry(self, abstime):
        """
        Looks up the record associated with a particular episode. If
        no records match the abstime, an empty recarray will be returned

        <abstime> - the time offset of the entry
        """
        table = self.elog.root.entries
        rnums = table.getWhereList(table.cols.abstime==int(abstime))
        return table.readCoordinates(rnums)

    def getfiles(self, abstime):
        """
        Returns the file/entry pairs associated with a particular episode.
        """
        table = self.elog.root.files
        rnums = table.getWhereList(table.cols.abstime==int(abstime))
        return table.readCoordinates(rnums)
       
    def getentrytimes(self):
        """
        Every entry has an abstime associated with it (the start of the
        episode).  All the records (for each channel) associated with
        an entry will have the same abstime.  This method returns all
        the abstime values.
        """
        table = self.elog.root.entries
        atimes = table.col('abstime')
        return atimes

    @property
    def channels(self):
        """
        Returns all the channels defined in the explog
        """
        return [r['name'] for r in self.elog.root.channels]

    @property
    def nchannels(self):
        return len(self.channels)

    @property
    def nentries(self):
        return self.getentrytimes().size

# end class explog

    
_reg_create = re.compile(r"'(?P<file>(?P<base>\w+)_\w+.pcm_seq2)' (?P<action>created|closed)")
_reg_triggeron = re.compile(r"TRIG_ON, (?P<chan>\w+):entry (?P<entry>\d*) \((?P<onset>\d*)\)")
_reg_triggeroff = re.compile(r"TRIG_OFF, (?P<chan>\w+):entry (?P<entry>\d*), wrote (?P<samples>\d*)")
_reg_stimulus = re.compile(r"stimulus: REL:(?P<rel>[\d\.]*) ABS:(?P<abs>\d*) NAME:'(?P<stim>\S*)'")
_reg_site = re.compile(r"site (?P<site>\d*)")
_reg_pen  = re.compile(r"pen (?P<pen>\d*)")


class Entries(t.IsDescription):
    """
    Table for entries (episodes)
    """
    abstime  = t.UInt64Col(pos=0)
    pen      = t.UInt16Col(pos=1)
    site     = t.UInt16Col(pos=2)
    duration = t.UInt32Col(pos=3)
    valid    = t.BoolCol(pos=4)

class Stimuli(t.IsDescription):
    abstime  = t.UInt64Col(pos=0)
    name     = t.StringCol(128, pos=1)
    entrytime= t.UInt64Col(pos=2)

class Files(t.IsDescription):
    """
    This table is used to look up the files associated with an entry
    """
    abstime  = t.UInt64Col(pos=0)
    channel  = t.UInt16Col(pos=1)
    filebase = t.StringCol(128,pos=2)
    entry    = t.UInt16Col(pos=3)

class Channels(t.IsDescription):
    name     = t.StringCol(64)


def readexplog(logfile, outfile, site_sort=False):
    """
    Parses episode information from the explog. Generates an h5
    file with the entry data in one table and the stimulus data in
    another.

    site_sort - if true, makes a directory for each pen/site and moves
                the files into that directory.

    Returns a new explog object
    """

    expdir = os.path.dirname(logfile)
    channels = []
    files  = {}
    triggers = {}
    currentpen   = 0
    currentsite  = 0
    lastabs      = 0
    absoffset    = 0

    # we scan through the file looking for pcm_seq2 files being opened; this
    # gives us the root filenames that we'll use in generating the indexed
    # label files. Trigger events specify the current entry. The stimulus events
    # don't get logged between TRIG_ON and TRIG_OFF lines, so we have to reconstruct
    # which entry they belong to later. So two tables are created, one keyed by
    # filename/entry with the start and stop times of each recording, and the
    # other keyed by the absolute start time of the stimuli


    fp = open(logfile)
    line_num = 0

    h5 = t.openFile(outfile, mode='w', title="parsed explog %s" % logfile,
                    filters=t.Filters(complevel=1, complib='zlib'))
    entries = _maketable(h5, h5.root,'entries',Entries)
    stimuli = _maketable(h5, h5.root,'stimuli',Stimuli)
    chantable = _maketable(h5, h5.root,'channels',Channels)
    ftbl = _maketable(h5, h5.root,'files',Files)

    for line in fp:
        line_num += 1

        if line.startswith("FFFF"):
            m1 = _reg_create.search(line)
            if m1:
                if m1.group('action')=='created':
                    # we try to locate the file either in the current directory
                    # or in a subdirectory site_<pen>_<site>
                    seqfile = m1.group('file')
                    subdir = "site_%d_%d" % (currentpen, currentsite)

                    if site_sort and os.path.exists(seqfile):
                        if not os.path.exists(subdir): os.mkdir(subdir)
                        print "%s -> %s" % (seqfile, subdir)
                        os.system("mv %s %s " % (seqfile, subdir))

                    if os.path.exists(os.path.join(subdir, seqfile)):
                        seqfile = os.path.join(subdir, seqfile)
                    files[m1.group('base')] = seqfile
                else:
                    files.pop(m1.group('base'))
            else:
                print "parse error: Unparseable FFFF line (%d): %s" % (line_num, line)

        # new pen or new site
        if line.startswith("IIII"):
            m1 = _reg_pen.search(line)
            m2 = _reg_site.search(line)
            if m1:
                currentpen = int(m1.group('pen'))
            elif m2:
                currentsite = int(m2.group('site'))

        # when saber quits or stop/starts, the abstime gets reset. Since stimuli are matched with triggers
        # by abstime, this can result in stimuli getting assigned to episodes deep in the past
        # The workaround for this is to maintain an offset that gets set to the most recent
        # abstime whenever a quit event is detected
        if line.startswith('%%%%'):
            if line.rstrip().endswith('start'):
                absoffset = lastabs
            else:
                o = line.find('add')
                if o > 0:
                    cname = line[o+4:-1]
                    try:
                        channels.index(cname)
                    except ValueError:
                        channels.append(cname)


        # trigger lines
        # this is kind of tricky.  Need to make one entry per trigger for
        # the files table, and one entry per abstime for the entry table
        if line.startswith("TTTT"):
            m1 = _reg_triggeron.search(line)
            m2 = _reg_triggeroff.search(line)
            if m1:
                # trigger on
                time_trig = int(m1.group('onset')) + absoffset
                triggers[(m1.group('chan'), int(m1.group('entry')))] = time_trig
                lastabs = time_trig
                #print "Entry %d starts %d" % (currententry, time_trig)
            elif m2:
                # trigger off
                key = (m2.group('chan'), int(m2.group('entry')))
                try:
                    trig_on = triggers[key]
                    n_samples = int(m2.group('samples'))
                    # check that we have a file defined in the files dict
                    if not files.has_key(key[0]):
                        print "parse error: found entry %s:%d but no file (line %d)" % \
                          (key + (line_num,))
                    else:
                        row = ftbl.row
                        row['abstime'] = trig_on
                        row['filebase'] = files[key[0]]
                        row['channel'] = channels.index(key[0])
                        row['entry'] = key[1]
                        row.append()
                        triggers.pop(key)
                        # when we've emptied the triggers dictionary, it's time to write the
                        # entry record
                        if len(triggers)==0:
                            row = entries.row
                            row['abstime'] = trig_on
                            row['pen'] = currentpen
                            row['site'] = currentsite
                            row['duration'] = n_samples
                            row['valid'] = True
                            row.append()

                except KeyError:
                    print "parse error: found TRIG_OFF for %s:%s, but missed TRIG_ON (line %d)" % \
                          (key + (line_num,))
                except IndexError:
                    print "Unable to parse channel number from %s (line %d)" % (key[0] % line_num)
                    triggers.remove(key)
            else:
                    print "parse error: Unparseable TTTT line (%d): %s" % (line_num, line)


        # stimulus lines
        if line.startswith("QQQQ"):
            m1 = _reg_stimulus.search(line)
            if m1:
                time_stim_rel = float(m1.group('rel'))
                time_stim_abs = int(m1.group('abs')) + absoffset
                lastabs = time_stim_abs
                stimname = m1.group('stim')
                # is it only triggered stimuli that start with "File="?
                # I'm going to leave this as general as possible at some memory cost;
                # the untriggered stimuli will get discarded
                if stimname.startswith('File='):
                    stimname = stimname[5:]

                row = stimuli.row
                row['abstime'] = time_stim_abs
                row['name'] = stimname
                row.append()
            else:
                print "parse error: Unparseable QQQQ line: %s" % line

    # done parsing file
    fp.close()

    for c in channels:
        row = chantable.row
        row['name'] = c
        row.append()

    # sync entries
    h5.flush()
    assignstimuli(h5)


    return explog(h5)

# end readexplog()

def assignstimuli(h5, cull_unrecorded=True):
    """
    Sets the stimulus entrytime values to match the corresponding
    entries in the entry table.

    If cull_unrecorded is true, the stimulus records that don't
    correspond to any recorded entry are removed from the database
    """
    entries = h5.root.entries

    start_times = entries.cols.abstime[:]
    stop_times = entries.cols.duration[:] + start_times

    i = 0
    stimuli = h5.root.stimuli
    for stimulus in stimuli:
        atime = stimulus['abstime']
        while i < len(stop_times):
            if atime < start_times[i]:
                stimulus['entrytime'] = 0L
                stimulus.update()
                break
            elif atime < stop_times[i]:
                stimulus['entrytime'] = long(start_times[i])
                stimulus.update()
                break
            else:
                i += 1

    stimuli.flush()
    
    if cull_unrecorded:
        # easiest just to copy the valid rows to a new table
        valid = stimuli.getWhereList(stimuli.cols.entrytime!=0)
        if len(valid)==0:
            print "Warning: no stimuli match the current site, keeping old list"
        else:        
            stimuli.rename('oldstimuli')
            newstimuli = _maketable(h5, '/', 'stimuli', Stimuli)
            newstimuli.append(stimuli[:][valid])
            newstimuli.flush()
            stimuli.remove()

    h5.flush()

def _maketable(file, base, name, descr):
    tbl = file.createTable(base, name, descr)
    tbl.flavor = 'numpy'
    return tbl



if __name__=="__main__":

    import getopt

    if len(sys.argv) < 2:
        print """
        explog.py [-s] [-o <outfile>] <explog>
        
        Run this script to parse a text explog into an h5 file. Optionally
        (with -s) sort pcm_seq2 files into directories by pen/site. By default
        the output .h5 file is <explog>.explog.h5; use the -o flag to specify
        something else.
        
        """
        sys.exit(-1)

    opts,args = getopt.getopt(sys.argv[1:],"so:")

    site_sort = False
    if len(args)== 0:
        print "Must supply an explog file for input"
        sys.exit(-1)
    infile = args[0]
    outfile = infile + ".h5"

    for o,a in opts:
        if o=='-s':
            site_sort = True
        if o=="-o":
            outfile = a

    if os.path.exists(outfile):
        os.remove(outfile)
        
    z = readexplog(infile, outfile, site_sort)
